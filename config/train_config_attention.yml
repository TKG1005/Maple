episodes: 10
lr: 0.0005
batch_size: 2048
buffer_capacity: 4096
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.02
ppo_epochs: 4
algorithm: ppo
reward: composite
reward_config: config/reward.yaml

# Attention Network Configuration
network:
  type: "attention"  # Use attention network
  hidden_size: 128
  use_2layer: true
  use_attention: true
  use_lstm: false  # Pure attention without LSTM
  attention_heads: 4  # Number of attention heads
  attention_dropout: 0.1  # Dropout rate for attention