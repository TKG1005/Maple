episodes: 10
lr: 0.0005
batch_size: 2048
buffer_capacity: 4096
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.01
ppo_epochs: 4
algorithm: ppo
reward: composite
reward_config: config/reward.yaml

# Hybrid Network Configuration (LSTM + Attention)
network:
  type: "attention"  # Use attention network type
  hidden_size: 128
  use_2layer: true
  use_attention: true  # Enable attention mechanism
  use_lstm: true  # Enable LSTM as well
  lstm_hidden_size: 128  # LSTM hidden state dimension
  attention_heads: 4  # Number of attention heads
  attention_dropout: 0.1  # Dropout rate for attention