episodes: 10
lr: 0.0005
batch_size: 2048
buffer_capacity: 4096
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.01
ppo_epochs: 4
algorithm: ppo
reward: composite
reward_config: config/reward.yaml

# Training configuration
parallel: 10
checkpoint_interval: 500
checkpoint_dir: "checkpoints"
tensorboard: true

# Team configuration
team: "random"  # Options: default, random
teams_dir: null  # Directory for random team mode

# Opponent configuration
opponent: null  # Single opponent type: random, max, rule
opponent_mix: "max:7,self:3"  # Mixed opponents: "random:0.3,max:0.3,self:0.4"

# Self-play win rate based opponent update
win_rate_threshold: 0.6  # Win rate threshold for updating opponent
win_rate_window: 100      # Number of recent battles to track

# Model management
load_model: null  # Path to model file to resume training from
save_model: model.pt  # Path to save final model

# Network architecture configuration
network:
  type: "lstm"  # Options: basic, lstm, attention
  hidden_size: 128
  use_2layer: true
  use_lstm: true
  use_attention: false
  lstm_hidden_size: 128
  attention_heads: 4
  attention_dropout: 0.1
