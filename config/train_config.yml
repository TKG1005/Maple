# Configuration presets:
# - For quick testing: episodes=1, parallel=5, lr=0.0001
# - For development: episodes=50, parallel=20, lr=0.0003  
# - For production: episodes=1000, parallel=100, lr=0.0003

# Training parameters
episodes: 1 # Number of episodes to train (1=test, 50=dev, 1000=production)
lr: 0.0003    # Learning rate (0.0001=conservative, 0.0003=standard, 0.001=aggressive)
batch_size: 8192
buffer_capacity: 400000  # Optimized for 2534-dim state space (7.56GB memory)
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.03
ppo_epochs: 4
algorithm: ppo
reward: composite
reward_config: config/reward.yaml

# Training configuration
parallel: 1              # Parallel environments (5=test, 20=dev, 100=production)
checkpoint_interval: 100  # Save checkpoint every N episodes
checkpoint_dir: "checkpoints"
tensorboard: true

# Battle Mode Configuration
battle_mode: "local"  # "local" (IPC) or "online" (WebSocket)

# Local mode configuration (for battle_mode: "local")
local_mode:
  max_processes: 4      # Maximum number of Node.js processes for IPC
  process_timeout: 300   # Process timeout in seconds
  reuse_processes: true  # Whether to reuse Node.js processes across battles
  ipc_script_path: "scripts/node-ipc-bridge.js"  # Path to IPC server script (override with MAPLE_NODE_SCRIPT)
  full_ipc: true        # Phase 4: Full IPC mode without WebSocket fallback

# Pokemon Showdown server configuration (for battle_mode: "online") 
pokemon_showdown:
  servers:
    - host: "localhost"
      port: 8000
      max_connections: 100  # Maximum parallel connections to this server


  # Use single server for development: comment out servers 2-
  # For production: adjust max_connections based on server performance

# Model loading configuration
reset_optimizer: false  # Reset optimizer state when loading a model (useful for device changes)

# Team configuration
team: "random"           # Options: default, random
teams_dir: "config/teams"  # Directory for random team mode

# Opponent configuration
opponent: null           # Single opponent type: random, max, rule, self
opponent_mix: "max:2,self:8"       # Mixed opponents (e.g., "random:0.3,max:0.3,self:0.4")

# Self-play win rate based opponent update
win_rate_threshold: 0.7  # Win rate threshold for updating opponent (0.6-0.8 recommended)
win_rate_window: 200       # Number of recent battles to track

# League Training configuration (prevents catastrophic forgetting)
league_training:
  enabled: true            # Enable league training (vs historical opponents)
  historical_ratio: 0.3    # Ratio of battles against historical opponents (0.0-1.0)
  max_historical: 5        # Maximum number of historical snapshots to keep
  
  # Historical opponent selection method:
  # - "uniform": Random selection from all historical opponents (equal probability)
  #   Example: snapshots [1,2,3,4,5] → each has 20% selection chance
  # - "recent": Prefer recent historical opponents (50% from newest half)
  #   Example: snapshots [1,2,3,4,5] → [4,5] are preferred, [1,2,3] less likely
  # - "weighted": Weight by recency (newer opponents have higher selection probability)
  #   Example: snapshots [1,2,3,4,5] → weights [1,2,3,4,5], so 5 is 5x more likely than 1
  selection_method: "recent"
  
  # Recommended settings:
  # - Early training: uniform (explore all historical strategies)
  # - Stable training: recent (focus on recent improvements)
  # - Fine-tuning: weighted (gradual transition from old to new)

# Model management
load_model: null  # Path to model file to resume training from
save_model: null  # Path to save final model

# Network architecture configuration  
network:
  type: "attention"  # Options: basic, lstm, attention, embedding
  hidden_size: 256
  use_2layer: true
  use_lstm: true        # Enable LSTM for sequential learning
  use_attention: true   # Enable attention mechanism
  lstm_hidden_size: 256
  attention_heads: 8    # More attention heads for complex patterns
  attention_dropout: 0.1
  
  # Pokemon Species Embedding Configuration (only used when type: "embedding")
  embedding_config:
    embed_dim: 32  # Embedding dimension (must be >= 6 for base stats)
    vocab_size: 1026  # 0 (unknown) + 1025 (Pokemon species)
    freeze_base_stats: true  # Whether to freeze base stats dimensions during training
    
    # Species indices in state vector (automatically detected if not provided)
    # Indices 836-847: my_team[0-5].species_id + opp_team[0-5].species_id
    species_indices: [836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847]

# Sequence learning configuration
sequence_learning:
  enabled: true    # Enable sequence-based learning for LSTM
  bptt_length: 0  # Truncated BPTT length (0=full episode, 50=recommended for long episodes)
  grad_clip_norm: 5.0  # Gradient clipping norm (2.0=conservative, 5.0=standard, 10.0=aggressive)

# Exploration strategy configuration (E-2 task)
exploration:
  epsilon_greedy:
    enabled: true            # Enable ε-greedy exploration wrapper
    epsilon_start: 1.0      # Initial exploration rate (1.0 = always explore)
    epsilon_end: 0.1        # Final exploration rate (0.05 = 5% exploration)
    decay_steps: 800       # Number of episodes/steps to decay from start to end (reasonable for training)
    decay_strategy: "exponential" # Decay strategy: "linear" or "exponential" (linear for predictable decay)
    decay_mode: "episode"    # Decay mode: "step" (per-action) or "episode" (per-episode)

# Scheduler configuration
scheduler:
  enabled: true  # Enable learning rate scheduler
  type: "step"    # Options: step, exponential, cosine, reduce_on_plateau
  
  # StepLR configuration
  step_size: 100  # Number of episodes between LR reductions
  gamma: 0.995       # Multiplicative factor for LR decay
  
  # ExponentialLR configuration (when type: "exponential")
  # gamma: 0.99     # Decay factor per episode
  
  # CosineAnnealingLR configuration (when type: "cosine")
  # T_max: 1000     # Maximum number of episodes for cosine cycle
  # eta_min: 0      # Minimum learning rate
  
  # ReduceLROnPlateau configuration (when type: "reduce_on_plateau")
  # mode: "min"     # "min" for loss, "max" for reward
  # factor: 0.7     # Factor by which LR will be reduced
  # patience: 30    # Number of episodes with no improvement after which LR will be reduced
  # verbose: true   # Print message when LR is reduced
