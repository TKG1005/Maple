# Default configuration for testing and short-term training
episodes: 1
lr: 0.0003
batch_size: 4096
buffer_capacity: 400000
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.02
ppo_epochs: 4
algorithm: ppo
reward: composite
reward_config: config/reward.yaml

# Training configuration
parallel: 20
checkpoint_interval: 500
checkpoint_dir: "checkpoints"
tensorboard: true

# Team configuration
team: "random"  # Options: default, random
teams_dir: null  # Directory for random team mode

# Opponent configuration
opponent: max  # Single opponent type: random, max, rule
opponent_mix: null  # Mixed opponents for testing

# Self-play win rate based opponent update
win_rate_threshold: 0.8  # Win rate threshold for updating opponent
win_rate_window: 100      # Number of recent battles to track

# Model management
load_model: null  # Path to model file to resume training from
save_model: null  # Path to save final model

# Network architecture configuration
network:
  type: "attention"  # Options: basic, lstm, attention, embedding
  hidden_size: 256
  use_2layer: true
  use_lstm: true
  use_attention: false
  lstm_hidden_size: 256
  attention_heads: 4
  attention_dropout: 0.1
  
  # Pokemon Species Embedding Configuration (only used when type: "embedding")
  embedding_config:
    embed_dim: 32  # Embedding dimension (must be >= 6 for base stats)
    vocab_size: 1026  # 0 (unknown) + 1025 (Pokemon species)
    freeze_base_stats: false  # Whether to freeze base stats dimensions during training
    
    # Species indices in state vector (automatically detected if not provided)
    # Indices 836-847: my_team[0-5].species_id + opp_team[0-5].species_id
    species_indices: [836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847]

# Sequence learning configuration
sequence_learning:
  enabled: true  # Enable sequence-based learning for LSTM
  bptt_length: 0  # 0 means full episode length, >0 for truncated BPTT
  grad_clip_norm: 2.0  # Gradient clipping norm
