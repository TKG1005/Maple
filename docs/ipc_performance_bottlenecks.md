# IPC モード高速化ボトルネックと対策まとめ

目的: ローカル IPC（Node.js ブリッジ経由）での学習実行を高速化し、CPU 使用率を高める。

対象範囲: `src/env/*`（IPC/プレイヤー/レジストリ周辺）, `train.py`（並列実行と学習ループ）, `src/agents/*`（推論呼び出し頻度）

---

## 現状ボトルネック（観測）

- IPC/アプリ層処理の固定コスト
  - Python↔Node 間 NDJSON エンコード/デコード、行単位ルーティング。
  - `asyncio` キュー入出力、RqID 整合チェック、整数→BattleOrder 変換の遅延実行。
  - room_tag マッピングや終了コールバック伝搬などの安全策ロジック。

- コントローラのライフサイクル
  - バトルごとに Node ブリッジを起動/終了（プロセス再利用が未接続）。
  - 起動/初期化/GC の繰り返しでスパイク的な遅延が発生。

- 計算処理（推論/更新）の支配
  - 毎ターン `policy_net`/`value_net` 前向き（Attention+LSTM 隠れ状態更新）。
  - エピソード後の PPO 更新（`ppo_epochs` 回）で CPU 時間を大きく消費。
  - 並列時は PyTorch スレッドとアプリ層スレッドが競合しやすい。

- ロギング/I/O
  - IPC raw 行ログや詳細デバッグ、TensorBoard 出力が定常コストに。

---

## 高速化方針（マルチスレッド/CPU 利用）

- PyTorch スレッド最適化（重要）
  - parallel=1: `torch.set_num_threads(物理コア数)`, `set_num_interop_threads` を小さめ。
  - parallel>1: `set_num_threads(max(1, cores // parallel))` で過剰スレッドを抑制。
  - 必要に応じて `OMP_NUM_THREADS`/`MKL_NUM_THREADS` を整合。

- バッチ推論（最優先）
  - 複数 env の観測/マスクを集約し、`B×D` の一括前向き＋一括サンプリング。
  - Python 呼び出し回数削減と BLAS 利用率向上で CPU 使用率を 70–90% へ。
  - 既存の `ThreadPoolExecutor` 駆動でも「集約→一括推論→反映」への変形が可能。

- 学習更新の最適化
  - 親プロセスで更新実施しつつ、`ppo_epochs` とスレッド数のバランスを調整。
  - 先行最適化は「前向きバッチ化」優先、更新の並列化は次段階で検討。

---

## 高速化方針（IPC 固有）

- コントローラ再利用/プール化（reuse）
  - Node ブリッジをバトルごとに起動せず、room_tag 切替や一プロセス多重化で使い回す。
  - `ControllerRegistry` にプール（上限=物理コア〜2×）を導入、空きコントローラに割り当て。
  - 起動/GC コスト削減、ウォームアップ状態維持。

- メッセージ粒度の最適化
  - 1 ターンあたりの往復を最小化。RqID 再計算やマッピング再構築の回数を抑制。
  - JSON 解析の高速化（Python 側 `orjson` 導入検討、Node 側不要ログ削減）。

- ログ削減
  - IPC raw 行ログや詳細デバッグを `INFO→WARNING` へ、TB はエピソード単位に集約。

---

## WebSocket 版との効果の違い（比較）

- WS 版
  - ネットワーク往復/サーバ処理が支配。ターン数短縮の効果が時間に直結。
  - Python 側 CPU 最適化の効果は相対的に小さめ。

- IPC 版
  - アプリ層/推論/更新が支配。CPU スレッド最適化・バッチ推論・プロセス再利用の効果が大きい。
  - 同一マシンでも WS は OS スタックのパイプライン化が効きやすく、IPC はアプリ層の固定コストが残りやすい。

---

## 計測計画（改善の可視化）

- プロファイリング
  - `--profile` 有効化で `env_step`/`optimizer_step` を分離記録（`PerformanceProfiler`）。
  - 1 エピソード時間の内訳: 環境実行 / 学習更新 / ログ I/O / 待機・同期。

- メトリクス
  - 平均ターン数（`episode_lengths`）と 1 ターン平均時間。
  - 推論呼び出し回数、バッチサイズ、CPU 使用率（top/psutil）。
  - IPC 送受信行数（1 ターンあたり）、Node プロセス起動回数。

- 再現テスト
  - ログレベル/ TB オンオフ、parallel の変更、スレッド数掃き出しで比較表を作成。

---

## 優先実装ロードマップ（推奨順）

1) バッチ推論導入（最優先）
   - 観測/マスクの集約→一括前向き→行動反映。
   - ベクトル化したサンプリング（`rng.choice` のバッチ版）。

2) コントローラ再利用/プール化
   - `ControllerRegistry` にプール API を追加し、既存コントローラの再利用を徹底。
   - room_tag 生成戦略の見直し（プロセス再利用を阻害しない設計）。

3) スレッド数最適化
   - `parallel` に応じた Torch スレッド数調整ユーティリティを追加（CPU/環境依存対応）。

4) ログ/JSON 最適化
   - デバッグログを段階的に抑制、必要に応じて `orjson` の導入を検討。

---

## 既知の落とし穴と注意点

- 過剰並列（スレッド/プロセス）は逆効果
  - 文脈スイッチ/キャッシュ汚染/スレッド争奪でスループット低下。

- 再現性と学習安定性
  - バッチ化で RNG の利用位置が変わるため、乱数シードとサンプリング順序の設計が必要。

- 監視とフォールバック
  - 改修後に異常を検知できるよう、異常系ログ（RqID 不一致、マスク異常）は保持。

---

## まとめ

IPC 版の高速化は、(1) 推論のバッチ化、(2) コントローラ再利用、(3) Torch スレッド数の最適化で大きな効果が期待できる。WS 版はネットワーク遅延の影響が大きく、同じ施策の効果は相対的に小さい。上記ロードマップに沿って段階導入し、プロファイリングで効果を定量確認することを推奨する。

