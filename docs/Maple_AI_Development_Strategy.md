
# MapleプロジェクトAI開発状況と改善戦略

## 現状のMaple AI

### 学習アルゴリズム
- 方策勾配法（REINFORCE, PPO）
- 自己対戦（相互学習）

### ニューラルネット構造
- 全結合層1層（128ユニット）
- 状態ベクトル入力 → 行動選択（11次元）

### 報酬設計
- 勝敗ベースのスパース報酬（勝利+10、敗北-10、途中0）

### 探索戦略
- PPOによるソフトマックス行動選択（探索補助弱い）

### 対戦相手
- 自己対戦のみ

## 他AIの成功例

| プロジェクト名/研究者 | アルゴリズム | ネットワーク構造 | 報酬設計 | 探索戦略 | 対戦相手 |
|---------------------|-------------|-----------------|---------|----------|----------|
| hsahovic氏          | Policy Gradient | シンプルMLP | 勝敗のみ | 標準的 | ランダム |
| Stanford CS230      | PPO         | 3層MLP (512ユニット) | 勝敗のみ | 標準的 | 段階的強化（ランダム→ミニマックス）|
| Caleb Lewis         | DQN         | シンプルMLP | 勝敗＋HP差 | ε-greedy | 固定相手 |
| Vivek Keval         | A2C         | シンプルMLP | 勝敗＋HP差等 | 標準的 | ランダム、MaxDamage |
| Foul Play           | Minimax探索 | - | ゲーム理論的評価関数 | 同時手探索 | 多様 |
| MIT Jett Wang       | オフラインRL（Transformer） | Transformerベース | 人間データ模倣＋強化学習 | 大量データ | 人間プレイヤー |

## Maple AIの課題
1. **モデル容量の不足**（浅いネットワーク構造）
2. **報酬信号のスパースさ**（クレジット割当困難）
3. **探索の弱さ**（局所的最適戦略に陥る可能性）
4. **不完全情報への対応不足**（長期的推論が困難）
5. **対戦相手の多様性不足**（自己対戦限定）
6. **戦略行動の未学習**（交代・選出判断が弱い）
7. **アルゴリズムのサンプル効率の低さ**（PPOの効率性問題）
8. **学習安定性・ハイパーパラメータ最適化不足**

## M7以降の改善方針

### A. 強化学習アルゴリズム改善
- PPOの修正・ハイパーパラメータ最適化
- A2CやDQNなど他のアルゴリズムを検討
- モデルベースのMCTSなどを将来的に導入

### B. ニューラルネットワーク構造強化
- 層の追加（例: 全結合層を3層）
- LSTMやGRUを追加し、履歴情報を活用
- Transformer型モデルの将来的検討

### C. 報酬設計改善
- 中間報酬の導入（例: 相手撃破時＋1、被撃破時－1）
- HP割合差を基にした報酬導入を慎重に検討
- 敗北時ペナルティの調整

### D. 探索戦略の強化
- エントロピー正則化を導入
- ε-greedy等の探索補助の追加
- 初期状態のランダム化

### E. 対戦相手の多様化
- カリキュラム学習（ランダム → MaxDamage → 過去モデル）
- 他の戦略を持つルールベースAI導入
- 世代間トーナメントでの自己強化

### F. 全般的な改善
- TensorBoardによるモニタリング強化
- 並列実行環境の導入
- 状態エンコーディング最適化

## 推奨開発戦略

1. 基盤改善（アルゴリズム・ネットワーク最適化）
2. 対戦相手の多様化（カリキュラム学習導入）
3. 報酬設計の慎重な改善（微小なshaping）
4. 推論時に探索アルゴリズムを導入（MCTSやミニマックスを活用）

以上を順次進め、段階的に強化することで、一定以上の強さを持つAIを目指す。
