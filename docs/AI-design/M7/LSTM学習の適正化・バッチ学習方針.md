# LSTM学習の適正化・バッチ学習方針（Mapleプロジェクト向け）

## 目的
- LSTMが長期依存関係を適切に学習できるようにする
- 複数エピソードのバッチ学習時にエピソード間で状態や勾配が混線しないようにする
- 長いBPTT長でも計算資源内で安定して学習できるようにする
- 勾配爆発を発生させない

---

## 実装指針

### 1. エピソード境界での隠れ状態リセット

**Why:**  
エピソード間でLSTMの隠れ状態や勾配伝播が混ざると、正しい系列学習ができないため。

**What:**  
- 各エピソード開始時にLSTMの隠れ状態（hidden, cell）をゼロクリア
- エピソード終了時、次エピソードで新規に状態初期化

**How:**  
- LSTMの隠れ状態はRLAgentやRunner等、エピソード制御層で保持
- `reset_hidden_states`のようなメソッドを用意し、環境リセット時に必ず呼び出す
- LSTMネットワーク自体は状態を持たない（ステートレス設計）

---

### 2. エピソード単位・系列長単位のシーケンシャル学習（BPTT設計）

**Why:**  
バッチ化の都合で時系列情報が途切れると、LSTMが長期依存を学習できないため。

**What:**  
- エピソードごとに「観測系列（軌跡）」をそのままLSTMへ入力
- 1エピソード全体 or セグメントごと（例: 20step）でBPTT実行（デフォルトは１エピソード全体で、configで変更できるように）

**How:**  
- データローダでは「[バッチサイズ, シーケンス長, 特徴次元]」形式でデータ整形
- エピソードごとに時系列順にLSTMへ入力し、エピソード内の系列で勾配を伝播
- セグメント分割型（Truncated BPTT）の場合、隠れ状態のみ継承・計算グラフはdetachで切断

---

### 3. 計算コストと実現性の検討

**Why:**  
長いシーケンス全体を学習するとメモリと計算量が急増するため。

**What:**  
- BPTT長を長くして学習できるかを検証
- メモリ消費が過大ならTruncated BPTT（区切り学習）を導入

**How:**  
- まずエピソード全体（例: 100step）で学習し、OOM等あれば20step前後で分割
- 分割時も隠れ状態のみシームレスに継承し、勾配はdetachで切る

---

### 4. 勾配爆発への対策

**Why:**  
長期BPTTや大きなモデルで勾配爆発が起きやすいため。

**What:**  
- 勾配クリッピングを標準導入

**How:**  
- PyTorchなら`torch.nn.utils.clip_grad_norm_`で、各更新step時にクリッピング
- クリッピング値は一般的に5.0〜10.0程度から検討
- 併せて学習率も低めに（例: 0.0005）

---

## コーディングチェックリスト

- [ ] LSTM隠れ状態をエピソード単位で初期化しているか
- [ ] シーケンスはエピソードごとに切れているか（異なるエピソード間で状態・勾配が繋がっていないか）
- [ ] データローダは「バッチ×系列長」形式か
- [ ] Truncated BPTTの分割・hiddenのdetach処理を適切にしているか
- [ ] 勾配クリッピングが有効になっているか
- [ ] 長期BPTTでOOMや学習崩壊が起きないか確認しているか

---

