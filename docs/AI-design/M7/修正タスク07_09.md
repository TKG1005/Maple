âº ğŸ“Š è©³ç´°åˆ†æçµæœ

  1. å‹ç‡100%ã®ä¸å…·åˆã«ã¤ã„ã¦

  âœ… å®Ÿéš›ã®å‹ç‡: 66.0%
  - å‹åˆ©: 660å› (66.0%)
  - æ•—åŒ—: 152å› (15.2%)
  - å¼•ãåˆ†ã‘: 188å› (18.8%)

  ğŸ” å•é¡Œã®ç‰¹å®š:
  pokemon_count > 0ã‚’å‹åˆ©æ¡ä»¶ã¨ã—ã¦ä½¿ã£ãŸãŸã‚ã€å¼•ãåˆ†ã‘ã‚‚å‹åˆ©ã«å«ã¾ã‚Œã¦ã„ãŸã€‚å®Ÿéš›ã®å‹ç‡ã¯66%ã§ã€å­¦ç¿’ã«ã‚ˆã‚Š49.2%â†’76.4%ã«æ”¹å–„ã—ã¦ã„ã¾ã™ã€‚

  2. Self-playå®Ÿè£…ã®å•é¡Œç‚¹

  âŒ é‡å¤§ãªè¨­è¨ˆå•é¡Œã‚’ç™ºè¦‹:
  # ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒã˜ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å…±æœ‰
  rl_agent = RLAgent(env, policy_net, value_net, optimizer, algorithm=algorithm)
  if opp_type == "self":
      opponent_agent = RLAgent(env, policy_net, value_net, optimizer, algorithm=algorithm)

  å•é¡Œ:
  - ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒã˜ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨
  - åŒã˜è¦³æ¸¬ã«å¯¾ã—ã¦åŒã˜è¡Œå‹•ã‚’é¸æŠ
  - å®Ÿè³ªçš„ã«ã€Œè‡ªåˆ†vsè‡ªåˆ†ã€ã®å¯¾æˆ¦
  - å­¦ç¿’åŠ¹æœãŒé™å®šçš„

  3. å­¦ç¿’ç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ¨å¥¨å¤‰æ›´

  ğŸ¯ ç¾åœ¨ã®è¨­å®š:

  lr: 0.002
  batch_size: 2048  # 1024ã‹ã‚‰å¤‰æ›´æ¸ˆã¿
  buffer_capacity: 4096

  ğŸ“ˆ æ¨å¥¨å¤‰æ›´:

  å­¦ç¿’ç‡ (Learning Rate)

  # ç¾åœ¨: 0.002 â†’ æ¨å¥¨: 0.0005-0.001 (å¤‰æ›´æ¸ˆã¿)
  lr: 0.0005  # ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’

  ç†ç”±:
  - ç¾åœ¨ã®å­¦ç¿’ç‡ã¯é«˜ã™ãã‚‹å¯èƒ½æ€§
  - å ±é…¬ã®å¤‰å‹•ãŒå¤§ãã„ï¼ˆå¤‰å‹•ä¿‚æ•°0.610ï¼‰
  - ã‚ˆã‚Šå°ã•ãªå­¦ç¿’ç‡ã§å®‰å®šåŒ–

  ãƒãƒƒãƒã‚µã‚¤ã‚º (Batch Size)

  # ç¾åœ¨: 1024 â†’ æ¨å¥¨: 512-2048
  batch_size: 2048  # ã‚ˆã‚Šå®‰å®šã—ãŸå‹¾é…æ¨å®šï¼ˆå¤‰æ›´æ¸ˆã¿)

  ç†ç”±:
  - å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‹¾é…æ¨å®šã‚’å®‰å®šåŒ–
  - PPOã§ã¯1024-4096ãŒä¸€èˆ¬çš„
  - è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨ç›¸è«‡ã—ã¦æ±ºå®š

  ãã®ä»–ã®èª¿æ•´:

  # PPOç‰¹æœ‰ã®èª¿æ•´
  ppo_epochs: 4      # ç¾åœ¨ã®ã¾ã¾
  clip_range: 0.2    # ç¾åœ¨ã®ã¾ã¾ã€å®‰å®šã—ã¦ã„ã‚‹
  entropy_coef: 0.01 # 0.02â†’0.01ã«æ¸›å°‘ï¼ˆæ¢ç´¢ã‚’æ¸›ã‚‰ã™,å¤‰æ›´æ¸ˆã¿ï¼‰

  4. å ±é…¬æ­£è¦åŒ–ã«ã¤ã„ã¦

  ğŸ“Š ç¾åœ¨ã®å ±é…¬åˆ†å¸ƒ:

  - å¹³å‡: 69.2ç‚¹
  - æ¨™æº–åå·®: 42.2ç‚¹
  - ç¯„å›²: -72.2 ã€œ 172.2ç‚¹

  ğŸ¯ å ±é…¬æ­£è¦åŒ–ã®æ–¹æ³•:

  1. Running Mean/Stdæ­£è¦åŒ–

  class RewardNormalizer:
      def __init__(self, epsilon=1e-8):
          self.running_mean = 0
          self.running_var = 1
          self.count = 0
          self.epsilon = epsilon

      def update(self, reward):
          self.count += 1
          delta = reward - self.running_mean
          self.running_mean += delta / self.count
          self.running_var += delta * (reward - self.running_mean)

      def normalize(self, reward):
          std = np.sqrt(self.running_var / max(1, self.count - 1))
          return (reward - self.running_mean) / (std + self.epsilon)

  2. ç§»å‹•å¹³å‡æ­£è¦åŒ–

  def normalize_reward(reward, window_size=100):
      # éå»100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å¹³å‡ãƒ»æ¨™æº–åå·®ã§æ­£è¦åŒ–
      if len(reward_history) >= window_size:
          mean = np.mean(reward_history[-window_size:])
          std = np.std(reward_history[-window_size:])
          return (reward - mean) / (std + 1e-8)
      return reward

  3. ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°æ­£è¦åŒ–

  def clip_normalize_reward(reward, clip_range=(-10, 10)):
      # å¤–ã‚Œå€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ã¦ã‹ã‚‰æ­£è¦åŒ–
      clipped = np.clip(reward, clip_range[0], clip_range[1])
      return clipped / clip_range[1]  # [-1, 1]ã«æ­£è¦åŒ–

  ğŸ”§ å®Ÿè£…æ¨å¥¨:

  1. Running Mean/Stdæ­£è¦åŒ–ã‚’æ¡ç”¨
  2. å ±é…¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥ã«æ­£è¦åŒ–
  3. GAE (Generalized Advantage Estimation) ã¨çµ„ã¿åˆã‚ã›

  # å®Ÿè£…ä¾‹
  normalizer = RewardNormalizer()
  normalized_reward = normalizer.normalize(raw_reward)

  ğŸ’¡ ç·åˆçš„ãªæ”¹å–„ææ¡ˆ

  1. ç·Šæ€¥å¯¾å¿œ:

  - Self-playã§ç‹¬ç«‹ã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨
  - å­¦ç¿’ç‡ã‚’0.0005ã«ä¸‹ã’ã‚‹

  2. ä¸­æœŸå¯¾å¿œ:

  - å ±é…¬æ­£è¦åŒ–ã®å®Ÿè£…
  - ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’2048ã«å¢—åŠ 
  - ã‚ˆã‚Šå¼·ã„å¯¾æˆ¦ç›¸æ‰‹ã®è¿½åŠ 

  3. é•·æœŸå¯¾å¿œ:

  - å¯¾æˆ¦ç›¸æ‰‹ãƒ—ãƒ¼ãƒ«ã®æ‹¡å¼µ
  - Meta-learningæ‰‹æ³•ã®å°å…¥
  - è‡ªå·±å¯¾æˆ¦ã®æ”¹å–„
