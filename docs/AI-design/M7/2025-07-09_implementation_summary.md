# 2025-07-09 å®Ÿè£…ã‚µãƒãƒªãƒ¼: è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ç·Šæ€¥ä¿®æ­£

## ğŸ“‹ å®Ÿè£…æ¦‚è¦

æœ¬æ—¥å®Ÿè£…ã—ãŸä¸»è¦ãªå¤‰æ›´ç‚¹ã¨ä¿®æ­£ã«ã¤ã„ã¦åŒ…æ‹¬çš„ã«ã¾ã¨ã‚ã¾ã™ã€‚

## ğŸ”§ å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### æ–°è¦ä½œæˆãƒ•ã‚¡ã‚¤ãƒ«
- `src/rewards/normalizer.py` - å ±é…¬æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ 
- `docs/AI-design/M7/self_play_design_fix.md` - è‡ªå·±å¯¾æˆ¦è¨­è¨ˆä¿®æ­£æ¡ˆ
- `docs/é–‹ç™ºæ—¥è¨˜/2025-07-09_è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ç·Šæ€¥ä¿®æ­£.md` - é–‹ç™ºæ—¥è¨˜

### ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«
- `train.py` - è‡ªå·±å¯¾æˆ¦ãƒ­ã‚¸ãƒƒã‚¯ã®æ ¹æœ¬çš„ä¿®æ­£
- `src/agents/RLAgent.py` - ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãªã—å¯¾å¿œ
- `src/algorithms/base.py` - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åŸºåº•ã‚¯ãƒ©ã‚¹ä¿®æ­£
- `src/algorithms/ppo.py` - PPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¿®æ­£
- `src/algorithms/reinforce.py` - REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¿®æ­£
- `src/env/pokemon_env.py` - å ±é…¬æ­£è¦åŒ–çµ±åˆ
- `src/rewards/__init__.py` - æ–°ã‚¯ãƒ©ã‚¹è¿½åŠ 
- `config/train_config.yml` - å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
- `config/reward.yaml` - å ±é…¬é‡ã¿èª¿æ•´

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- `CLAUDE.md` - æ–°æ©Ÿèƒ½ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®èª¬æ˜è¿½åŠ 
- `README.md` - å¤‰æ›´å±¥æ­´ã®æ›´æ–°
- `docs/TODO_M7.md` - å®Œäº†ã‚¿ã‚¹ã‚¯ã®æ›´æ–°

## ğŸ¯ è§£æ±ºã—ãŸå•é¡Œ

### 1. è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ã®æ ¹æœ¬çš„æ¬ é™¥
```python
# å•é¡Œã®ã‚ã£ãŸã‚³ãƒ¼ãƒ‰
opponent_agent = RLAgent(env, policy_net, value_net, optimizer, algorithm=algorithm)
```
**å•é¡Œ**: ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒã˜ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å…±æœ‰

```python
# ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰
opponent_policy_net = create_policy_network(...)
opponent_value_net = create_value_network(...)
opponent_policy_net.load_state_dict(policy_net.state_dict())
opponent_value_net.load_state_dict(value_net.state_dict())

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å‡çµ
for param in opponent_policy_net.parameters():
    param.requires_grad = False
for param in opponent_value_net.parameters():
    param.requires_grad = False

opponent_agent = RLAgent(env, opponent_policy_net, opponent_value_net, None, algorithm)
```

### 2. å­¦ç¿’ã®ä¸å®‰å®šæ€§
- **å­¦ç¿’ç‡**: 0.002 â†’ 0.0005
- **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1024 â†’ 2048
- **ãƒãƒƒãƒ•ã‚¡å®¹é‡**: 2048 â†’ 4096
- **ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°**: 0.02 â†’ 0.01

### 3. å ±é…¬ã‚¹ã‚±ãƒ¼ãƒ«ã®å•é¡Œ
- **å ±é…¬æ­£è¦åŒ–**: RewardNormalizerã‚¯ãƒ©ã‚¹å®Ÿè£…
- **å®Ÿè¡Œçµ±è¨ˆ**: Welfordã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä½¿ç”¨
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥**: ç‹¬ç«‹ã—ãŸæ­£è¦åŒ–å™¨

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„

### å˜ä¸€ãƒ¢ãƒ‡ãƒ«åæŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
```
ä¸»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (å­¦ç¿’) â†â†’ å¯¾æˆ¦ç›¸æ‰‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (å‡çµ)
    â†“                           â†‘
  é‡ã¿æ›´æ–°                  é‡ã¿ã‚³ãƒ”ãƒ¼
    â†“                           â†‘
æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›              æ¯ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ›´æ–°
```

### å­¦ç¿’ãƒ•ãƒ­ãƒ¼
1. **ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹**: ä¸»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¾åœ¨ã®é‡ã¿ã‚’å¯¾æˆ¦ç›¸æ‰‹ã«ã‚³ãƒ”ãƒ¼
2. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‡çµ**: å¯¾æˆ¦ç›¸æ‰‹ã®requires_grad = False
3. **è‡ªå·±å¯¾æˆ¦å®Ÿè¡Œ**: ä¸»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆvså‡çµå¯¾æˆ¦ç›¸æ‰‹
4. **å­¦ç¿’æ›´æ–°**: ä¸»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿å­¦ç¿’
5. **æ¬¡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: å¯¾æˆ¦ç›¸æ‰‹ã«æ–°ã—ã„é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼

## ğŸ“Š å ±é…¬æ­£è¦åŒ–ã‚·ã‚¹ãƒ†ãƒ 

### RewardNormalizerã‚¯ãƒ©ã‚¹
```python
class RewardNormalizer:
    def __init__(self, epsilon=1e-8):
        self.running_mean = 0.0
        self.running_var = 1.0
        self.count = 0
        self.epsilon = epsilon
    
    def update(self, reward):
        # Welfordã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        self.count += 1
        delta = reward - self.running_mean
        self.running_mean += delta / self.count
        delta2 = reward - self.running_mean
        self.running_var += delta * delta2
    
    def normalize(self, reward):
        if self.count <= 1:
            return reward
        std = np.sqrt(self.running_var / (self.count - 1))
        return (reward - self.running_mean) / (std + self.epsilon)
```

### çµ±åˆæ–¹æ³•
```python
# PokemonEnv._calc_reward()
raw_reward = self._composite_rewards[pid].calc(battle) + win_reward

if self.normalize_rewards and pid in self._reward_normalizers:
    self._reward_normalizers[pid].update(raw_reward)
    normalized_reward = self._reward_normalizers[pid].normalize(raw_reward)
    return float(normalized_reward)

return raw_reward
```

## ğŸ”§ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¿®æ­£

### å‹ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œ
```python
# BaseAlgorithm
def update(self, model: nn.Module, optimizer: torch.optim.Optimizer | None, batch: Dict[str, torch.Tensor]) -> float:
```

### PPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
if optimizer is not None:
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
return float(loss.detach())
```

### REINFORCEã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
```python
if optimizer is not None:
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
return float(loss.detach())
```

### RLAgentä¿®æ­£
```python
def __init__(self, env, policy_net, value_net, optimizer: torch.optim.Optimizer | None, algorithm):
    self.optimizer = optimizer  # Noneã‚‚è¨±å¯

def update(self, batch):
    if self.optimizer is None:
        return 0.0  # å‡çµã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å­¦ç¿’ã—ãªã„
    return self.algorithm.update(self.policy_net, self.optimizer, batch)
```

## âš™ï¸ è¨­å®šæœ€é©åŒ–

### train_config.yml
```yaml
episodes: 10
lr: 0.0005        # 0.002 â†’ 0.0005
batch_size: 2048  # 1024 â†’ 2048
buffer_capacity: 4096  # 2048 â†’ 4096
gamma: 0.997
gae_lambda: 0.95
clip_range: 0.2
value_coef: 0.6
entropy_coef: 0.01  # 0.02 â†’ 0.01
ppo_epochs: 4
algorithm: ppo
```

### reward.yaml
```yaml
fail_immune:
  weight: 1.5  # 1.0 â†’ 1.5
  enabled: true
pokemon_count:
  weight: 0.5  # 1.0 â†’ 0.5
  enabled: true
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœ

### æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```bash
Testing updated self-play implementation with correct dimensions...
âœ“ Successfully created independent networks
âœ“ Policy net 1 params: 57163
âœ“ Policy net 2 params: 57163
âœ“ Networks are different objects: True
âœ“ Main agent update loss: 0.4565
âœ“ Opponent agent update loss: 0.0000 (should be 0.0)
All tests passed!
```

### å ±é…¬æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ
```bash
Testing reward normalizer...
Raw: 1.00, Normalized: 1.00
Raw: 2.00, Normalized: 0.41
Raw: -1.00, Normalized: -0.99
Raw: 3.00, Normalized: 0.97
Raw: 0.50, Normalized: -0.38
Stats: {'mean': 1.1, 'std': 1.597, 'count': 5}
```

## ğŸ“ˆ æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### 1. å­¦ç¿’ã®å®‰å®šåŒ–
- å ±é…¬æ­£è¦åŒ–ã«ã‚ˆã‚Šå­¦ç¿’ãŒå®‰å®š
- é©åˆ‡ãªå­¦ç¿’ç‡ã§åæŸæ€§å‘ä¸Š
- ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ ã§å‹¾é…æ¨å®šå®‰å®š

### 2. çœŸã®è‡ªå·±å¯¾æˆ¦
- ä¸»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ®µéšçš„ã«å¼·åŒ–
- å¯¾æˆ¦ç›¸æ‰‹ã¯ç¾åœ¨ã®å®ŸåŠ›ã‚’åæ˜ 
- å˜ä¸€ã®æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›

### 3. è¨ˆç®—åŠ¹ç‡å‘ä¸Š
- 1ã¤ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®ã¿æ›´æ–°
- å¯¾æˆ¦ç›¸æ‰‹ã¯æ¨è«–ã®ã¿
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–

## ğŸ”„ Gitå±¥æ­´

### ã‚³ãƒŸãƒƒãƒˆä¸€è¦§
```bash
cbb970100 Fix self-play learning architecture for proper single-model convergence
8ee53b8d8 Implement urgent fixes for self-play training system
```

### å¤‰æ›´çµ±è¨ˆ
```
20 files changed, 292 insertions(+), 16113 deletions(-)
```

## ğŸ¯ ä»Šå¾Œã®å±•æœ›

### çŸ­æœŸç›®æ¨™
1. ä¿®æ­£ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã§ã®å­¦ç¿’å®Ÿé¨“
2. å ±é…¬æ­£è¦åŒ–ã®åŠ¹æœæ¸¬å®š
3. é•·æœŸå­¦ç¿’ã§ã®å®‰å®šæ€§ç¢ºèª

### ä¸­æœŸç›®æ¨™
1. å¤šæ§˜ãªè‡ªå·±å¯¾æˆ¦æ‰‹æ³•ã®æ¤œè¨
2. ãƒ¡ã‚¿å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å®Ÿè£…
3. ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ã®å°å…¥

### é•·æœŸç›®æ¨™
1. äººå£ãƒ™ãƒ¼ã‚¹å­¦ç¿’ã®å®Ÿè£…
2. è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸¦è¡Œå­¦ç¿’
3. é«˜åº¦ãªè‡ªå·±å¯¾æˆ¦æˆ¦ç•¥ã®é–‹ç™º

## ğŸ“š å‚è€ƒæ–‡çŒ®

- Welford's Online Algorithm for Running Statistics
- PPO (Proximal Policy Optimization) Paper
- Self-Play in Reinforcement Learning Literature
- Reward Normalization Techniques in Deep RL

## ğŸ ã¾ã¨ã‚

æœ¬æ—¥ã®ç·Šæ€¥ä¿®æ­£ã«ã‚ˆã‚Šã€Mapleã®è‡ªå·±å¯¾æˆ¦ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã®ç‚¹ã§å¤§å¹…ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸï¼š

1. **çœŸã®è‡ªå·±å¯¾æˆ¦**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‹¬ç«‹æ€§ã®ç¢ºä¿
2. **å­¦ç¿’å®‰å®šåŒ–**: å ±é…¬æ­£è¦åŒ–ã¨è¨­å®šæœ€é©åŒ–
3. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„**: å˜ä¸€ãƒ¢ãƒ‡ãƒ«åæŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
4. **æŠ€è¡“çš„å“è³ª**: å‹å®‰å…¨æ€§ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

ã“ã‚Œã‚‰ã®æ”¹å–„ã«ã‚ˆã‚Šã€ã‚ˆã‚ŠåŠ¹æœçš„ã§å®‰å®šã—ãŸå¼·åŒ–å­¦ç¿’ãŒå¯èƒ½ã«ãªã‚Šã€Mapleãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å“è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚