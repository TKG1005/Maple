# B-1: RandomBot（ランダム行動ボット）の実装
ステップ 1: クラスファイルの作成 – 新規に bots/random_bot.py を作成し、RandomBot クラスを定義します
。このクラスは既存の MapleAgent（またはそのサブクラス）を継承し、対戦相手としてランダムに行動するbotのロジックを持たせます。
ステップ 2: ランダム行動ロジックの実装 – RandomBot クラス内に select_action メソッドを実装します。ここでは有効な行動マスクからランダムに1つのインデックスを選びます。既存の RandomAgent のコードを参考に、numpy.where でマスクが真のインデックスを取得し、rng.choice でランダム選択して返すようにします
。また、万一有効行動がない場合はインデックス0を返すフェールセーフも入れます。
ステップ 3: act メソッドの追加 – select_action で得た行動インデックスをそのまま返す act メソッドを実装します（RandomAgent と同様）
。これにより、エージェントが観測とマスクから直接行動を選択できるようになります。
ステップ 4: トレーニングコードへの統合 – トレーニングスクリプトで RandomBot を使用するように変更します。train_rl.py および evaluate_rl.py の create_opponent_agent 関数にケースを追加し、引数が "random" のとき新規作成した RandomBot(env) を返すよう修正します
。これにより、従来 RandomAgent によって実装されていたランダム対戦相手を、RandomBot クラスで置き換えます。
ステップ 5: オプションパラメータの追加 – （必要に応じて）対戦相手をランダムbotと自己対戦を切り替える比率を指定するパラメータを導入します
。これは後述の B-3 で実装する複数対戦相手スケジューラの布石です。例えば --opponent random（固定）に加え、比率指定用に --opponent-mix オプションを追加します。ここではまだ実装しませんが、インターフェースとして文字列で比率を受け取れるよう引数を用意しておきます。
B-2: MaxDamageBot（最大火力ボット）の実装
ステップ 1: クラスファイルの作成 – 新規に bots/max_damage_bot.py を作成し、MaxDamageBot クラスを定義します
。このクラスも MapleAgent を継承し、最もダメージの出る行動を選択するルールベースAIとなります（poke-envのMaxBasePowerプレイヤーと同等のロジック）

ステップ 2: 最大火力行動ロジックの実装 – MaxDamageBot.select_action メソッド内で、現在選択可能なすべての攻撃技を評価し、その中から威力が最大のものを選びます。具体的には、battle.available_moves（現在出せる技のリスト）から各技の base_power を取得し、Pythonの組み込み関数maxで最大威力の技を見つけます
。見つかった技オブジェクトを対応する行動インデックスに変換する必要があるため、action_helper.get_available_actions_with_details 等で得られる技IDと行動マッピングを使用し、その技IDに対応する行動インデックスを特定して返します
。もし利用可能な攻撃技が存在しない場合（交換のみ可能な状況やすべて変化技の場合）は、ランダムに有効な行動を選ぶフォールバック処理を入れておきます
（例えば有効な行動インデックス一覧からランダム選択）。
ステップ 3: 対戦相手エージェント生成処理への組み込み – train_rl.py および evaluate_rl.py の create_opponent_agent 関数において、opponent_type == "max" の場合に MaxDamageBot(env) を生成・返却するケースを追加します
。これにより、コマンドライン引数で "--opponent max" を指定した際に最大火力ボットとの対戦環境が構築できます（※argparseの--opponentオプションのchoicesにも "max" を追加しておきます
）。
ステップ 4: 動作検証 – 実装後、短いエピソードで挙動を確認します。例えば5戦程度、学習エージェントを MaxDamageBot と対戦させ、既存の RandomBot 相手との結果と勝率を比較しログに出力します
。評価スクリプトで --opponent max を指定して実行すれば、eval_YYYYMMDD_HHMMSS.log に各試合の結果が記録されます。勝率差がログに反映されるよう、対戦終了毎に勝敗を集計して出力する処理を加えると良いでしょう（5戦中何勝かを表示するなど）
。これにより、ランダム相手に比べ最大火力相手で学習エージェントの性能がどう変化するか確認できます。
B-3: 学習相手スケジューラの実装（複数対戦相手のミックス）
ステップ 1: 引数パーサの拡張 – トレーニング実行時に複数種別の対戦相手を混在させるため、--opponent-mix オプションを新設します
。例えば "random:0.3,max:0.3,self:0.4" のように文字列で各ボットと割合を指定できるようにし、argparse.ArgumentParser に該当オプションを追加します（train_rl.py の引数定義に追記）
。このオプションが指定された場合は、既存の --opponent オプション（単一相手指定）は無視される設計で構いません。
ステップ 2: 比率文字列のパース処理 – train/opponent_pool.py ファイルを作成し、文字列で与えられた相手比率を解析する関数を実装します。例えば parse_opponent_mix(mix_str: str) -> List[Tuple[str, float]] のような関数で、入力 "random:0.3,max:0.3,self:0.4" をカンマ区切りで分解し、それぞれ "random",0.3 といったペアに変換します
。パース時には、比率の合計が1.0になるかチェックし、不足または超過があれば正規化します。また、不正な相手タイプ文字列が含まれていればエラーを出すようにします（有効なキーは "random", "max", "rule", "self" など事前に定義）。
ステップ 3: 対戦相手タイプ選択ロジックの実装 – 上記で得た相手タイプと比率のリストに基づき、各エピソード開始時にどの相手と戦わせるかをランダムに選択する処理を実装します。例えば OpponentPool クラスを作り、その中に sample_opponent_type() メソッドを持たせて重み付き乱数で1つのタイプを返すようにします。Pythonのrandom.choices（またはnumpy.random.choice）に比率を渡して選択すると実装が簡潔です。この処理をトレーニングループ内で各エピソードごとに呼び出し、対戦相手タイプを決定します。
ステップ 4: 環境の再初期化とエージェント登録 – エピソードごとに選択された対戦相手タイプに基づき、新たな対戦環境を構築します。既存の init_env(opponent_type) 関数を拡張し、引数に応じて都度環境を生成するようにします（あるいは init_env を毎回呼び出す）
。具体的には、前エピソードで使用した env を env.close() でクローズし、次のエピソード用に PokemonEnv を新規作成します。その際、選択された相手タイプに応じて:
ランダム/ルール/MaxDamage 相手: create_opponent_agent で対応するbotインスタンスを生成し、env.register_agent(opponent_agent, "player_1") で環境のプレイヤー1に登録
。
自己対戦（self）相手: 学習エージェント自身を相手とします。この場合、プレイヤー0とプレイヤー1の両方に学習用エージェントを割り当てます。実装方法としては、新たな RLAgent インスタンス（あるいは既存エージェントのコピー）を作成し、同一のネットワークパラメータを共有させます。例えばメインの RLAgent の持つ policy_net と value_net を共有（あるいはクローン）した第二の RLAgent を opponent_agent として生成します（更新は行わないため、DummyAlgorithm を割り当てるか、optimizerを共有しても良い）
。そしてプレイヤー1にこのエージェントを登録します。結果として、環境内で両者が同じ方策を用いて対戦する“自己対戦”環境が実現します。
環境生成後は、忘れずに学習エージェント（RLAgent）の登録も行います。新規に作った env に対して、env.register_agent(main_rl_agent, "player_0") を呼び出し、学習エージェントがプレイヤー0として環境と紐付くようにします
（RLAgentの env 属性も新しい環境に差し替える）。以上の手順により、各エピソードで動的に異なる対戦相手との環境セットアップが完了します。
ステップ 5: トレーニングループの修正 – エピソードループ内で環境と相手が都度変わるため、env.reset() から env.step() に至る処理を汎用化します。環境がシングルエージェントモード（SingleAgentCompatibilityWrapper の場合、すなわち相手がランダムbotで従来方式）か、マルチエージェントモード（相手がルールベース/MaxDamage/自己対戦の場合）かで処理を分けます。例えば:
シングルエージェント環境の場合: 従来と同様に obs, info, mask = env.reset(return_masks=True) で初期観測を得て、info["request_teampreview"] がTrueなら agent.choose_team でチーム選択コマンドを送り
、以降はループで action = agent.act(obs, mask) を計算し env.step(action, return_masks=True) を呼ぶ標準的な単一エージェントの流れとします
。
マルチエージェント環境の場合: obs_dict, info, masks = env.reset(return_masks=True) で両プレイヤーの初期観測を取得します
。もしチームプレビューが要求されたら、学習エージェントと相手エージェント双方の choose_team を呼び、それぞれのコマンドを辞書に入れてenv.step({"player_0": cmd0, "player_1": cmd1})で送信します
。メインループでは各ステップごとに、環境から返された観測 observations とマスク masks から、学習エージェントの行動と相手エージェントの行動をそれぞれ取得し、env.step({"player_0": action0, "player_1": action1}, return_masks=True) を呼び出します
。相手がルールベースやMaxDamageの場合はそれぞれのクラスのact/select_actionが呼ばれ、相手が自己対戦の場合はプレイヤー1側のRLAgent（コピー）が自分のネットワークで行動選択を行います。環境からは各プレイヤーのreward辞書やdoneフラグが返るため、学習側（player_0）の報酬・終了フラグを確認しつつループを進めます
。
ステップ 6: 後処理とログ – 各エピソード終了時に、そのエピソードで対戦に使用した相手タイプをログに記録するとデバッグに役立ちます（例: "Episode X finished - Opponent: MaxDamageBot, Result: Win" のように出力）。環境を都度作り直した場合は、env.close() を呼んで非同期タスクや接続をクリーンアップします。最後に全エピソード終了後、必要なら勝率や報酬平均などを集計して表示します（複数相手が混在する場合、相手別の成績も分析できると良いでしょう）。