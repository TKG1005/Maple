# Maple Project M6 Backlog – PPO導入と強化学習アルゴリズムの汎用化

## 目的

- 現行の REINFORCE 手法による強化学習エージェントをベースに、近接方策最適化 (PPO) アルゴリズムを導入して学習性能・安定性を向上させる  
- 新旧アルゴリズム（REINFORCE vs PPO）の学習結果を比較評価し、改善効果を検証する  
- 強化学習エージェント周辺の設計を見直し、将来的に A2C や SAC など他のアルゴリズムにも容易に差し替え可能な柔軟性を持つ基盤を構築する  
- 全体として、学習アルゴリズムを設定に応じて切り替えられる拡張性・保守性の高いフレームワークを実現する  

## バックログ（全 18 ステップ）

各ステップは「Goal (WHAT) ▸ Done (DONE) ▸ Test (HOW) ▸ Tech (WITH)」で整理しています。

---

### Step 1 : 環境インタフェース整理

- **Goal**  
  強化学習環境 PokemonEnv の観測・報酬・終了フラグのインタフェースを見直し、エージェント側で一貫して利用できるよう整備する  

- **Done**  
  `env.reset()` や `env.step()` で得られる各プレイヤーの観測・報酬・終了情報が仕様書通りの形式（例： `obs`, `rewards`, `terminated`, `info` の dict 構造）で取得可能  

- **Test**  
  `train.py --dry-run` 実行時に環境初期化から終了まで一連の情報が欠損なく得られ、ログに各プレイヤーのキーが正しく含まれることを確認  

- **Tech**  
  Gymnasium / PokemonEnv / Python  

---

### Step 2 : アルゴリズム切替機構設計

- **Goal**  
  強化学習アルゴリズム（REINFORCE, PPO など）を切り替えて適用できるコード構造にリファクタリングする  

- **Done**  
  RLAgent や学習スクリプトの構成がアルゴリズムごとの処理を抽象化した設計に変更され、新しいアルゴリズムクラスを追加しても既存コードの大部分を修正せず利用可能  

- **Test**  
  簡易的なダミーアルゴリズム実装をプラグインして学習ループを実行し、環境との入出力処理を共通コードで再利用できることをコードレビューで確認  

- **Tech**  
  コード設計 / リファクタリング  

---

### Step 3 : 方策・価値ネットワーク構造分離

- **Goal**  
  PPO 導入に備え、方策を出力する `PolicyNetwork` と状態価値を推定する `ValueNetwork` をそれぞれ実装し、エージェント内で管理する  

- **Done**  
  `PolicyNetwork(observation_space, action_space)` および `ValueNetwork(observation_space)` がインスタンス化でき、前者は行動の確率分布を、後者は与えられた観測の価値Vをそれぞれ出力可能  

- **Test**  
  `RLAgent` 初期化時に両ネットワークが正しく生成・保持されること、かつダミー観測に対して `policy_net.forward(obs)`, `value_net.forward(obs)` を呼び出し期待どおりのサイズの出力（確率ベクトル・スカラー値）が得られることを確認  

- **Tech**  
  PyTorch / numpy  

---

### Step 4 : GAE（一般化アドバンテージ推定）導入

- **Goal**  
  エピソード終了後の報酬系列から方策勾配の分散低減のための Advantage 値を計算する処理を追加する（GAE: Generalized Advantage Estimation の実装）  

- **Done**  
  各エピソードの全ステップについて、割引報酬と `ValueNetwork` 出力を用いて `Advantage` を算出できる関数が実装される（例： `compute_gae(rewards, values)` が各時刻の Advantage 配列を返す）  

- **Test**  
  簡単な報酬シーケンスを入力した場合に `Advantage` 計算結果が手計算と一致することをユニットテストで確認（例：割引率1.0・λ=1.0の場合、通常のモンテカルロ法による累積報酬差分と一致）  

- **Tech**  
  Python（数学計算） / NumPy  

---

### Step 5 : PPOクリップ損失関数実装

- **Goal**  
  PPOアルゴリズムの方策勾配損失を計算する処理を実装する（確率比のクリッピング対応を含む）  

- **Done**  
  古い方策確率と新しい方策確率、および `Advantage` を入力として、クリップ付近で勾配が飽和するクリップ付き目的関数が計算される。価値関数の損失とエントロピー項も組み合わせて最終的な損失を算出可能  

- **Test**  
  事前に用意した遷移データについて、クリッピング有無で損失計算結果が変化することを確認。例えば、ある行動の確率が大きく変動した場合でもクリップ範囲内では `Advantage` 符号に応じて損失が減少し、範囲を超える変動では損失に大差がないことをロギングで検証  

- **Tech**  
  PyTorch（autograd） / 数式実装  

### Step 6 : PPOエージェント学習ループ実装

- **Goal**  
  自己対戦学習スクリプトに PPO 更新処理を組み込み、一定ステップ分のデータ収集後に複数エポックにわたりネットワーク更新を行う学習ループを実装する  

- **Done**  
  `train_rl.py` をもとに`train.py` を作成し、実行時にエピソードごとの軌跡データをバッファに蓄積し、エピソード終了ごとに設定したエポック数だけ Step5の損失関数でネットワークを更新する処理が追加される
  （例： `--ppo-epochs 4` 指定時に各エピソードで4回のパラメータ更新を実施）  

- **Test**  
  PPOモードで数エピソード学習させた際に、ログ上で各エピソードに複数回の更新が行われていること（損失値やネットワークパラメータがエポックごとに変化していること）を確認。学習前後で方策の出力分布に変化が生じていることを簡易評価する  

- **Tech**  
  Python ループ制御 / PyTorch Optimizer  

---

### Step 7 : ハイパーパラメータ設定拡張（PPO）

- **Goal**  
  PPOアルゴリズム用のハイパーパラメータを設定ファイルやコマンドライン引数から指定できるようにする（例：クリップ率・GAE λ・更新エポック数・価値関数係数など）  

- **Done**  
  `config/train_config.yml` や `train.py --help` に PPO 固有の主要パラメータが追加され、ユーザが値を変更可能。既定値は REINFORCE 相当の挙動（例：クリップ無効化など）になるよう設定  

- **Test**  
  各パラメータを極端な値に変更して学習を実行し、挙動が変化することを確認（例： `--clip 0.0` でクリッピングを無効化した場合に方策が大きく更新されやすくなる、 `--gae-lambda 0.0` で Advantage が単純割引和になる 等）  

- **Tech**  
  argparse / PyYAML  

---

### Step 8 : 並列環境対応（PPOサンプル収集）

- **Goal**  
  PPO学習のサンプル効率を高めるため、複数の対戦環境を並行実行してデータを収集する仕組みを導入する  

- **Done**  
  `--parallel N` のようなオプションで同時に N 試合を進行させ、各環境から遷移を収集してまとめて学習更新を行えるようになる。例えば2並列環境でエピソードあたりの実行時間がシングル環境より短縮される  

- **Test**  
  `--parallel 2` を指定して10エピソード学習した際の実行時間が、 `--parallel 1` （デフォルト）に比べておおむね半分程度になることをログのタイムスタンプや外部計測で確認。学習結果（損失の傾向や報酬）は並列・非並列で大きな差異がないことを確認する  

- **Tech**  
  マルチスレッド / Gymnasium VectorEnv / 非同期処理  

---

### Step 9 : モデル保存・読み込み拡張

- **Goal**  
  PPO 導入に伴い増えたネットワーク構成（ポリシー＋価値ネット）を適切に永続化できるようモデル保存機能を拡張する  

- **Done**  
  `train.py --save model.pt` 実行時に方策ネットワークと価値ネットワークの重みが両方保存される（例：1つの `.pt` ファイルに辞書形式で2ネットワーク分の `state_dict` を格納）。また `evaluate_rl.py` 側でも保存ファイルから両ネットワークを読み込み、推論用に方策ネットワークを復元できる  

- **Test**  
  PPO モードで学習→保存したモデルファイルを用い、従来の RLAgent （または新たなPPO対応のエージェントクラス）でロードして自己対戦を1エピソード実行し、エラー無く動作することを確認。保存前後で同一入力に対する方策の出力が一致することをアサートする  

- **Tech**  
  PyTorch `torch.save` / `torch.load` / ファイル入出力  

---

### Step 10 : アルゴリズム選択オプション追加

- **Goal**  
  学習スクリプト実行時に使用するアルゴリズムを選択できるオプションを追加する  

- **Done**  
  `train.py` 実行時に `--algo reinforce` や `--algo ppo` の引数でアルゴリズムを切り替え可能。指定された値に応じて、従来のモンテカルロ方策勾配（REINFORCE）か新実装の PPO 手法で学習が行われる  

- **Test**  
  各アルゴリズムを1エピソードずつ実行し、ログや学習挙動が切り替わっていることを確認  
  （例：REINFORCEでは毎エピソード終了時に1度だけ更新、PPOでは複数エポック更新や Advantage 利用が行われている旨のログが出力される）  

- **Tech**  
  Python CLI / argparse  

### Step 11 : 評価スクリプト拡張（PPO対応）

- **Goal**  
  対戦評価用スクリプト evaluate_rl.py を拡張し、PPO で学習したエージェントのモデルも読み込んで対戦評価できるようにする  

- **Done**  
  `python evaluate_rl.py --model my_ppo_model.pt --n 5` のように PPOエージェントの重みファイルを指定して実行した場合でも、内部で適切に PolicyNetwork が復元されて自己対戦または対戦相手とのバトルが最後まで完了する  

- **Test**  
  上記コマンドで5戦実行し、勝敗結果や平均報酬が従来と同様に表示されることを確認。PPO 学習済みモデル vs ランダムエージェント等の組み合わせで対戦させ、エラー無く動作することを検証する  

- **Tech**  
  PokemonEnv / RLAgent / MapleAgent (Random)  

---

### Step 12 : 学習スクリプト自動テスト（PPO）

- **Goal**  
  CI（pytest）にて PPOアルゴリズムで1エピソード学習 するテストケースを追加し、基本的な学習処理がエラー無く完了することを保証する  

- **Done**  
  `pytest -k test_train_selfplay_one_episode_ppo` のようなテストが新設され、 subprocess 経由で `train.py --algo ppo --episodes 1` を実行して正常終了（コード0）することを確認する  

- **Test**  
  CI上で上記テストが PASS し、ログに1エピソード分の学習完了メッセージ（損失値や対戦結果の出力など）が含まれることを確認。REINFORCE 用のテストと併せて両アルゴリズムでテスト網羅できていることを担保する  

- **Tech**  
  pytest / subprocess / GitHub Actions  

---

### Step 13 : PPOエージェント対戦検証

- **Goal**  
  PPOで学習したエージェントを用いた対戦が期待通り進行し、報酬や終了判定が正しく機能することを検証する  

- **Done**  
  例として PPO学習済みモデル vs ランダムエージェント で5戦対戦させ、全試合が正常終了する。勝者には +10、敗者に -10 の報酬が与えられ、引き分け時は双方0になるなど、報酬・終了処理が既定どおりであることを確認

- **Test**  
  `evaluate_rl.py --model ppo_model.pt --opponent random --n 5` を実行し、終了後に出力される各試合の勝敗結果および rewards ・ terminated の内容が正当な値になっていることをアサートする（ログ中に勝率や平均報酬が算出される場合はその値も確認）  

- **Tech**  
  PokemonEnv / MapleAgent (Random) / RLAgent  

---

### Step 14 : REINFORCE vs PPO比較実験

- **Goal**  
  現行のREINFORCEアルゴリズムと新実装のPPOアルゴリズムの性能を比較評価するため、同条件で学習実験を行いデータを収集する  

- **Done**  
  それぞれのアルゴリズムで一定エピソード学習を実行し、エピソードごとの報酬や最終的な勝率など主要な指標を記録する（例：各エピソードの平均報酬をログ出力する機能を追加し、学習曲線データを保存）。両者の結果データが揃う  

- **Test**  
  REINFORCE・PPO各方式で例えば100エピソードずつ学習を走らせ、ログファイルや `logs/` ディレクトリにそれぞれの報酬推移データが記録されていることを確認。同一環境・条件下で両アルゴリズムの学習結果データを取得できていることを担保する  

- **Tech**  
  logging / 実験管理  

---

### Step 15 : 比較結果の図示

- **Goal**  
  ステップ14で取得したログデータを分析し、両アルゴリズムの学習曲線や最終性能を可視化して比較する  

- **Done**  
  REINFORCE vs PPO のエピソードごとの平均報酬や勝率推移を1つのグラフにプロットし、傾向の違いを示す図を作成する。必要に応じてエージェント同士の直接対戦結果などもまとめ、考察を加える  

- **Test**  
  生成されたグラフ画像において、例えば PPO 手法の曲線が REINFORCE より早期に収束している、最終的な勝率が高い、あるいは分散が小さい等の特徴が読み取れることを確認。分析結果をレビューし、アルゴリズム改良の効果が妥当であると評価される  

- **Tech**  
  Matplotlib / Jupyter Notebook（分析） / Markdown（レポート）  

### Step 16 : 他アルゴリズム対応設計

- **Goal**  
  今後 A2C（Advantage Actor-Critic）や SAC（Soft Actor-Critic）など他の強化学習アルゴリズムにも対応できるよう、必要な拡張点を整理・設計する  

- **Done**  
  同期型のActor-Critic手法（例：A2C）やオフポリシー手法（例：SAC）の導入にあたり、共通化すべきインタフェースや再利用できるコンポーネント（リプレイバッファ、ネットワーク構造など）が明確になる。例えば、離散/連続行動やオンポリシー/オフポリシーの違いを吸収できるクラス設計の草案が作成される  

- **Test**  
  設計レビューにて、新アルゴリズム追加時の変更箇所が最小限に抑えられていることを確認。仮に AlgorithmBase のサブクラスとして A2C を実装する想定を立てた場合に、環境やエージェントの既存コードを大きく変更せず組み込めることを説明できる  

- **Tech**  
  設計検討 / アーキテクチャレビュー  

---

### Step 17 : ドキュメント更新

- **Goal**  
  本フェーズ（M6）での強化学習アルゴリズム拡張に関する手順や使用方法をドキュメントに反映する  

- **Done**  
  新たに `docs/M6_setup.md` が作成され、PPOアルゴリズムで学習を行う手順、アルゴリズム選択オプションの説明、並列実行の方法、比較実験の結果サマリなどが追記される。併せて `README` や既存ドキュメント中の該当部分も最新の内容に更新される  

- **Test**  
  ドキュメントの手順に従って環境構築～PPO学習・評価まで一通り実行できることを第三者が検証し、必要な情報が漏れなく記載されているとレビューで承認される  

- **Tech**  
  Markdown / 図表作成  

---

### Step 18 : M6 完了レビュー

- **Goal**  
  上記すべてのステップ完了後、強化学習基盤が複数アルゴリズムを柔軟に切り替えて運用できることを最終確認する  

- **Done**  
  全ての単体・統合テストが PASS し、REINFORCE・PPO 双方で安定して自己対戦学習・評価が可能であることをチームレビューで承認。作成した比較レポートをもとに、「Maple の強化学習エージェントはアルゴリズムモジュールを差し替えて性能向上を図れる」 状態を実証する  

- **Test**  
  コードリーディングおよびデモ実行により、アルゴリズム切替オプションやPPO学習の動作、他アルゴリズム拡張の見通しについてチェックリスト全項目を満たすことを確認し、M7 の成果物を承認する  

- **Tech**  
  総合レビュー / 関係者合意  

### 参考リポジトリ・資料
- RLAgent.py(https://github.com/TKG1005/Maple/blob/af7948de455ed5f2d09222c90d3958f4f9e0f771/src/agents/RLAgent.py) – 強化学習エージェントクラス（方策・学習処理を実装）
- Stable-Baselines3 PPO(https://github.com/DLR-RM/stable-baselines3) 実装 – オープンソース実装による PPO アルゴリズムの参考資料 