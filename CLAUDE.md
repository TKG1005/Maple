# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Maple is a Pokemon reinforcement learning framework built on top of `poke-env` and Pokemon Showdown. It implements multi-agent self-play training for Pokemon battles using deep reinforcement learning algorithms (PPO, REINFORCE). The framework features advanced infrastructure optimizations including multi-server distribution system, team caching with 37.2x speedup, automated server management, and comprehensive performance analysis tools.

## Priority rule
- ä¾‹å¤–ã‚„ã‚¨ãƒ©ãƒ¼ã«å¯¾ã—ã¦ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œã‚‰ãšã€ã‚¨ãƒ©ãƒ¼ã®åŸå› ãŒç‰¹å®šã§ãã‚‹ãƒ­ã‚°å‡ºåŠ›ã‚’ã™ã‚‹ã‚ˆã†ã«å®Ÿè£…ã—ã¦ã€‚ï¼ˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒã‚¨ãƒ©ãƒ¼ãªãå‹•ãã“ã¨ã‚ˆã‚Šã‚‚ã€ã‚¨ãƒ©ãƒ¼ã®åŸå› ãŒç‰¹å®šã§ãã‚‹ã“ã¨ã‚’å„ªå…ˆã—ã¦ã€ã‚¨ãƒ©ãƒ¼ãŒç‰¹å®šã§ããŸã‚‰ä¿®æ­£ã—ã¦ã„ã„ã‹ç¢ºèªã‚’æ±‚ã‚ã¦ã€‚ï¼‰


## Core Architecture

### Main Components

- **PokemonEnv**: Multi-agent environment that interfaces with Pokemon Showdown server via WebSocket with server configuration support
- **MapleAgent**: Base agent class for battle decision making
- **EnvPlayer**: Bridge between poke-env's Player class and PokemonEnv
- **StateObserver**: Converts battle state into numerical feature vectors for ML models
- **Reward System**: Modular reward components (knockouts, turn penalties, fail/immune actions, Pokemon count difference)
- **Algorithms**: PPO and REINFORCE implementations with GAE for policy gradient methods

### Infrastructure Components (2025-07-25)

- **MultiServerManager**: Load balancing system for distributed Pokemon Showdown servers
- **TeamCacheManager**: Global team caching system with 37.2x performance improvement
- **Server Management Scripts**: Automated server lifecycle management with PID tracking
- **Performance Analysis Tools**: Bottleneck identification and parallel efficiency testing
- **Async Action Processing**: Phase 1 & 2 parallelization for action processing and battle state retrieval

### Key Architecture Patterns

- **Async-Sync Bridge**: Uses `asyncio.run_coroutine_threadsafe()` to integrate poke-env's async WebSocket handling with synchronous RL training loops
- **Multi-Agent Dict Interface**: Environment returns observations, actions, rewards as dictionaries keyed by player ID
- **Modular Rewards**: CompositeReward class combines multiple reward components with configurable weights
- **Action Masking**: Valid actions are computed dynamically and passed to agents to prevent invalid moves
- **Multi-Server Load Balancing**: Automatic distribution of parallel environments across multiple Pokemon Showdown servers
- **Global Team Caching**: Thread-safe team data caching with 37.2x performance improvement
- **Process Management**: PID-based server tracking with graceful shutdown and automatic recovery
- **Parallel Step Processing**: Async methods for concurrent action processing and battle state retrieval (10-15% speedup)

## Development Commands

### Infrastructure Management
```bash
# Multi-server infrastructure (60x faster than manual setup)
./scripts/showdown start 5          # Start 5 servers (ports 8000-8004)
./scripts/showdown status           # Monitor server status and performance
./scripts/showdown quick            # Auto-start based on train_config.yml
./scripts/showdown stop             # Graceful shutdown of all servers

# Performance analysis and optimization
python benchmark_train.py           # Analyze training bottlenecks
python parallel_benchmark.py --parallel 5 10 15 --device cpu  # Test parallel efficiency
```

### Training
```bash
# Multi-server training with team caching (37.2x speedup)
python train.py  # Uses config/train_config.yml with development defaults

# Quick testing (override config for minimal training)
python train.py --episodes 1 --parallel 5

# Development training (balanced settings with team caching)
python train.py --episodes 50 --parallel 20

# Large-scale production training (multi-server distribution)
python train.py --episodes 1000 --parallel 100

# Resume training from checkpoint
python train.py --load-model checkpoints/checkpoint_ep5000.pt --episodes 100

# Training with specific network architectures
python train.py --network-type embedding --episodes 50  # Pokemon species embedding
python train.py --network-type attention --episodes 50  # Attention networks

# League training (anti-catastrophic forgetting)
# Enabled by default in config - trains against historical opponents
python train.py --episodes 100  # Uses league_training.enabled: true

# CPU training (recommended for Mac Silicon compatibility)
python train.py --device cpu --episodes 50 --parallel 20

# Legacy individual parameter training (still supported)
python train.py --algo ppo --episodes 100 --lr 0.0003 --team random
```

### Testing
```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_composite_reward.py
pytest -m slow  # Long-running tests
```

### Evaluation
```bash
# Basic model evaluation against random opponent
python evaluate_rl.py --model checkpoints/checkpoint_ep14000.pt --opponent random --n 10

# Evaluate with random teams for varied battles
python evaluate_rl.py --model checkpoints/checkpoint_ep14000.pt --opponent rule --team random --n 5

# Head-to-head model comparison
python evaluate_rl.py --models checkpoints/model_a.pt checkpoints/model_b.pt --n 10

# Evaluation with replay saving and custom team directory
python evaluate_rl.py --model checkpoints/checkpoint_ep14000.pt --opponent max --team random --teams-dir config/teams --replay-dir my_replays --n 3

# Plot training results comparison
python plot_compare.py
```

## Configuration Files

- `config/train_config.yml`: **Unified training configuration** with preset options for testing/development/production including multi-server setup
- `config/train_config_dev.yml`: Development-optimized configuration with single server and reduced resource usage
- `config/reward.yaml`: Reward component weights and enablement flags
- `config/env_config.yml`: Environment settings
- `config/action_map.yml`: Action space configuration
- `config/state_spec.yml`: State observation specification

## Key Implementation Details

### Environment Synchronization
The system uses a complex synchronization mechanism between async poke-env and sync RL training:
1. EnvPlayer monitors `battle.last_request` changes to detect server updates
2. Battle objects are passed to PokemonEnv only after request ID changes
3. Action queues coordinate between RL agents and poke-env players

### State Representation
StateObserver creates feature vectors from battle state including:
- Pokemon stats, moves, types
- Team composition and status
- Type effectiveness calculations
- Battle field conditions

### Reward Engineering
Multiple reward components can be combined:
- `KnockoutReward`: Rewards for fainting opposing Pokemon
- `TurnPenaltyReward`: Penalizes long battles
- `FailAndImmuneReward`: Penalizes failed and immune moves (default penalty: -0.02)
- `PokemonCountReward`: End-game rewards based on remaining Pokemon difference (1 diff: 0 pts, 2 diff: Â±2 pts, 3+ diff: Â±5 pts)

## Testing Strategy

The project uses pytest with custom markers:
- Regular unit tests for reward functions and utilities
- `@pytest.mark.slow` for integration tests involving actual battles
- Separate test directories: `test/` for integration, `tests/` for unit tests

### Key Test Suites
- **Îµ-greedy Exploration**: `tests/test_epsilon_greedy_wrapper.py` (18 test cases)
  - Initialization, decay strategies, exploration/exploitation behavior
  - Statistics tracking and episode reset functionality
  - Mock-based testing for deterministic exploration validation
- **Move Embedding System**: `tests/test_move_embedding_*.py` 
  - Embedding generation, learnable mask consistency, performance optimization
- **Network Architecture**: `tests/test_*_networks.py`
  - Policy/value networks, LSTM/Attention implementations
- **Reward Components**: `tests/test_*_reward.py`
  - Individual and composite reward function validation

## Pokemon Showdown Integration

The project includes a full Pokemon Showdown server in `pokemon-showdown/` directory. The environment connects to `localhost:8000` by default for battle simulation.

## Model Architecture

- **Policy Network**: Maps state observations to action probabilities
- **Value Network**: Estimates state values for advantage calculation
- **Shared Features**: Both networks can share initial layers for efficiency
- **Action Masking**: Output layer respects valid action constraints
- **Pokemon Species Embedding**: Converts Pokemon species IDs to dense embeddings initialized with base stats

## Common Development Patterns

When modifying the codebase:
1. New reward components should inherit from `RewardBase` and be registered in `CompositeReward.DEFAULT_REWARDS`
2. Algorithm implementations should inherit from `BaseAlgorithm`
3. State features should be documented in `state_spec.yml`
4. Configuration changes should update corresponding YAML files
5. Integration tests should be marked with `@pytest.mark.slow`

### FailAndImmuneReward Implementation Details (Updated 2025-07-16)
The `FailAndImmuneReward` class provides penalties for invalid actions:
- Monitors `battle.last_fail_action` and `battle.last_immune_action` flags
- Default penalty: -0.3 (configurable via `config/reward.yaml`)
- Stateless design - no internal state to reset
- Enabled via `config/reward.yaml` with `fail_immune.enabled: true`
- **Fixed 2025-07-16**: CustomBattle implementation resolves missing flag issue

### Training Resume Functionality (train.py)
The `--load-model` option enables resuming training from checkpoints:
- Supports both new format (`{"policy": ..., "value": ...}`) and legacy format (single state dict)
- Automatically extracts episode number from filenames like `checkpoint_ep14000.pt`
- Continues episode numbering from the extracted number
- Example: `python train.py --load-model checkpoints/checkpoint_ep5000.pt --episodes 100`

### Random Team System
The `--team random` option enables varied training and evaluation:
- Each player independently selects a random team from `config/teams/` directory
- Teams are selected per battle, ensuring variety across episodes
- Custom team directory can be specified with `--teams-dir`
- Supports both training (`train.py`) and evaluation (`evaluate_rl.py`)
- Team files must be in Pokemon Showdown format

### Enhanced Player Naming (evaluate_rl.py)
Player names are automatically made unique to prevent NameTaken errors:
- Uses timestamp and random numbers to ensure uniqueness
- Names are truncated to 18 characters (Pokemon Showdown limit)
- Model names and agent types are preserved for replay identification
- Format: `ModelName_UniqueID` (e.g., `checkpoint_3418161`, `RuleBasedP_3418161`)

### Opponent Mixing System
The `--opponent-mix` option allows training against multiple opponent types:
- Format: `"type1:ratio1,type2:ratio2,type3:ratio3"`
- Example: `"random:0.3,max:0.3,self:0.4"` (30% random, 30% max damage, 40% self-play)
- Supports `random`, `max`, `rule`, and `self` opponent types
- Randomly selects opponent type based on specified ratios for each episode

### Self-Play Architecture (Updated 2025-07-09)
The self-play system implements a **single-model convergence approach**:
- **Main Agent**: Learns and updates through training, uses optimizer
- **Opponent Agent**: Uses frozen copy of main agent's current weights
- **Weight Copying**: Opponent networks are refreshed with main agent weights each episode
- **Network Freezing**: Opponent networks have `requires_grad=False` and no optimizer
- **Final Output**: Single trained model representing the learned policy
- **Learning Dynamics**: Main agent progressively improves by playing against its own current strength

### Reward Normalization System (New 2025-07-09)
Comprehensive reward normalization for stable training:
- **RewardNormalizer**: Running mean/std normalization with Welford's algorithm
- **WindowedRewardNormalizer**: Sliding window-based normalization (alternative)
- **Per-Agent Normalization**: Independent normalizers for each agent
- **Integration**: Automatic normalization in `PokemonEnv._calc_reward()`
- **Configuration**: Enable/disable via `normalize_rewards` parameter
- **Statistics**: Access normalization stats via `env.get_reward_normalization_stats()`

### LSTM Hidden State Management Fix (New 2025-07-10)
Fixed critical issue where LSTM networks were not maintaining sequential learning capability:

**Problem**: `RLAgent.select_action()` did not pass hidden states to LSTM networks, causing them to reinitialize on every action selection. This defeated the purpose of LSTM's sequential learning.

**Solution**: 
- Modified `RLAgent` to detect LSTM/Attention networks with `has_hidden_states` flag
- Added `reset_hidden_states()` method to reset hidden states at episode boundaries
- Updated `select_action()` to pass hidden states to LSTM networks
- Added tensor dimension handling for batch processing
- Modified training loops to call `reset_hidden_states()` at episode start

**Implementation Details**:
```python
# In RLAgent.__init__()
self.has_hidden_states = hasattr(policy_net, 'use_lstm') and (policy_net.use_lstm or (hasattr(policy_net, 'use_attention') and policy_net.use_attention))
self.policy_hidden = None
self.value_hidden = None

# In RLAgent.select_action()
if self.has_hidden_states:
    if obs_tensor.dim() == 1:
        obs_tensor = obs_tensor.unsqueeze(0)
    logits, self.policy_hidden = self.policy_net(obs_tensor, self.policy_hidden)
    if logits.dim() == 2 and logits.size(0) == 1:
        logits = logits.squeeze(0)

# In RLAgent.get_value() (New 2025-07-10)
if self.has_hidden_states:
    if obs_tensor.dim() == 1:
        obs_tensor = obs_tensor.unsqueeze(0)
    value, self.value_hidden = self.value_net(obs_tensor, self.value_hidden)
    if value.dim() == 2 and value.size(0) == 1:
        value = value.squeeze(0)

# In run_episode() and run_episode_with_opponent()
agent.reset_hidden_states()  # Called at episode start
```

**Value Network Hidden State Management (New 2025-07-10)**:
- Added `RLAgent.get_value()` method to handle value network hidden states
- Modified training loops to use `agent.get_value()` instead of direct `value_net()` calls
- Both policy and value networks now maintain separate hidden states
- Unified interface for both network types through RLAgent

**Testing**: Added comprehensive tests (`test_lstm_sequential_learning.py`) to verify:
- Hidden states change across action selections
- Hidden states are reset properly at episode boundaries
- Sequential learning produces different outputs for different histories
- LSTM networks behave differently from basic networks

### Configuration File System (New 2025-07-10)
Implemented comprehensive YAML-based configuration management to simplify training execution:

**Problem**: Training required long command lines with many parameters, making it difficult to manage different training scenarios and reproduce experiments.

**Solution**: Created YAML configuration system with two main templates:
- `config/train_config.yml`: Testing and short-term training (10 episodes, mixed opponents, LSTM network)
- `config/train_config_long.yml`: Long-term training (1000 episodes, self-play, attention network)

**Features**:
- All training parameters configurable via YAML
- Command line arguments override config file values
- Network architecture configuration in config files
- Detailed configuration logging
- Parameter validation and type conversion

**Usage Examples**:
```bash
# Testing and short-term training
python train.py --config config/train_config.yml

# Long-term production training
python train.py --config config/train_config_long.yml

# Config file with parameter override
python train.py --config config/train_config.yml --episodes 20 --lr 0.001
```

### Win Rate-Based Opponent Update System (New 2025-07-10)
Implemented intelligent opponent update system to reduce excessive network copying in self-play:

**Problem**: Traditional self-play copied opponent networks every episode, leading to inefficient learning due to constantly changing opponents.

**Solution**: Conditional opponent updates based on win rate threshold:
- Monitor recent battle results using win_loss reward component
- Update opponent network only when win rate exceeds threshold (default 60%)
- Maintain opponent snapshots for consistent training
- Configurable win rate threshold and monitoring window

**Implementation**:
```python
def should_update_opponent(episode_num, battle_results, window_size, threshold):
    """Check if opponent should be updated based on recent win rate."""
    if len(battle_results) < window_size:
        return False
    recent_results = battle_results[-window_size:]
    wins = sum(1 for result in recent_results if result == 1)
    win_rate = wins / len(recent_results)
    return win_rate >= threshold
```

**Configuration**:
```yaml
# Self-play win rate based opponent update
win_rate_threshold: 0.6  # Win rate threshold for updating opponent
win_rate_window: 50      # Number of recent battles to track
```

**Benefits**:
- Reduced network copying frequency
- More stable learning against consistent opponents
- Improved training efficiency
- Configurable thresholds for different scenarios

### Network Forward Method Compatibility Fix (New 2025-07-10)
Fixed compatibility issue between basic networks and enhanced LSTM/Attention networks:

**Problem**: Basic networks (PolicyNetwork, ValueNetwork) only accept one argument in forward(), while enhanced networks (LSTM, Attention) accept optional hidden state parameters.

**Solution**: Added conditional forward method calls based on network capabilities:
```python
# Call value network with hidden state only if supported
if hasattr(value_net, 'hidden_state'):
    val0_tensor = value_net(obs0_tensor, value_net.hidden_state)
else:
    val0_tensor = value_net(obs0_tensor)
```

This ensures compatibility across all network types while maintaining optimal performance for each architecture.

## Project-Specific Rules

### Code Style and Conventions
- Use `from __future__ import annotations` for type hints
- All new agent classes must inherit from `MapleAgent` base class
- Use `MapleAgent.select_action(observation, action_mask)` interface for agents
- Implement both `select_action()` and `act()` methods for agent compatibility
- Import order: stdlib â†’ third-party â†’ local imports with `# noqa: E402` for late imports

### Agent Development Rules
- All agents must handle empty action masks gracefully
- Use `self.env.rng` for reproducible random number generation
- Agent constructors should accept `env` as first parameter
- Register agents with environment using `env.register_agent(agent, player_id)`
- Battle state access should use `self._get_current_battle()` pattern
- **RLAgent Optimizer**: Can accept `None` optimizer for frozen agents (self-play opponents)
- **Algorithm Updates**: All algorithms must handle `None` optimizer gracefully

### Testing Requirements
- Mark slow integration tests with `@pytest.mark.slow`
- Unit tests for reward functions must test edge cases (empty battles, missing attributes)
- Agent tests should verify both valid action selection and error handling
- Configuration changes require corresponding test updates

### Documentation Updates
- Update `README.md` changelog for new features
- Mark completed tasks in `docs/TODO_M7.md`
- Update this `CLAUDE.md` with new implementation details
- Include usage examples for new command-line options

### Prohibited Patterns
- Never use `poke_env.Player` directly - always use `MapleAgent` base class
- Avoid hardcoded paths - use relative paths from project root
- Don't create agents without proper environment registration
- Never ignore action masks - always respect valid action constraints
- **Self-Play**: Never create two learning agents in self-play (use frozen opponent instead)
- **Reward Normalization**: Don't reset normalizers between episodes (maintains running stats)
- **LSTM Networks**: Never store hidden states in network instances (use agent-level management)
- **Device Transfer**: Always check device compatibility before tensor operations
- **Parallel Training**: Never share mutable hidden states between threads

## Recent Updates

### Performance Profiling System Implementation (2025-07-28)
åŒ…æ‹¬çš„ãªæ€§èƒ½åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã€ThreadPoolExecutorç’°å¢ƒã§ã®æ­£ç¢ºãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚

#### Hybrid Profiling Architecture
**Problem**: ThreadPoolExecutorã«ã‚ˆã‚‹ä¸¦åˆ—å®Ÿè¡Œã§åŒã˜ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã«é‡è¤‡è¨ˆæ¸¬ãŒç™ºç”Ÿã—ã€çµ±è¨ˆãŒä¸æ­£ç¢ºã«ãªã£ã¦ã„ã¾ã—ãŸï¼ˆenv_step: 1108%ç­‰ï¼‰ã€‚

**Solution**: 
- **ãƒ¡ã‚¤ãƒ³é–¢æ•°ãƒ¬ãƒ™ãƒ«**: å…¨ä½“æ™‚é–“ã¨ThreadPoolExecutorå‡¦ç†ã®è¨ˆæ¸¬
- **ä»£è¡¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è©³ç´°è¨ˆæ¸¬**: æœ€åˆã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®1ç’°å¢ƒã®ã¿ã§è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- **ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•è¨­è¨ˆ**: `get_global_profiler()`ã«ã‚ˆã‚‹å®‰å…¨ãªä¸¦åˆ—ã‚¢ã‚¯ã‚»ã‚¹

#### Key Performance Metrics
**Environment Operations**: 
- `env_step`: 11.7% - Pokemon Showdowné€šä¿¡ãŒæœ€é‡è¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- `env_reset`: 2.2% - ç’°å¢ƒåˆæœŸåŒ–å‡¦ç†

**Learning Operations**: 
- `gradient_calculation`: 35.1% - GAEè¨ˆç®—ã¨ãƒãƒƒãƒå‡¦ç†
- `optimizer_step`: 33.9% - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°å‡¦ç†

**Agent Operations**:
- `agent_action_selection`: 1.2% - æ¨è«–å‡¦ç†ï¼ˆè»½é‡ï¼‰
- `agent_value_calculation`: 0.8%

#### Configuration System Integration
å…¨31é …ç›®ã®è¨­å®šèª­ã¿è¾¼ã¿ã‚’å®Œå…¨å¯¾å¿œã—ã€`batch_size`èª­ã¿è¾¼ã¿å•é¡Œã‚’ä¿®æ­£ï¼š
```python
# train.py ã§ã®çµ±åˆè¨­å®šèª­ã¿è¾¼ã¿
batch_size = int(cfg.get("batch_size", 4096))
buffer_capacity = int(cfg.get("buffer_capacity", 800000))
```

#### Usage and Documentation
```bash
# åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
python train.py --profile --profile-name session_name

# å‡ºåŠ›: logs/profiling/reports/session_name_timestamp_summary.txt
```

è©³ç´°ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: `docs/profiling-system.md`

### Latest Implementations (2025-07-25)

#### ğŸš€ Async Action Processing (10-15% Performance Boost)
å®Œå…¨ãªAsync Action Processingå®Ÿè£…ã«ã‚ˆã‚Šã€ç’°å¢ƒã®step()å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã—ã€è¿½åŠ ã®10-15%æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚

**Phase 1: Action Processing Parallelization**
- `_process_actions_parallel()`: ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å‡¦ç†ã‚’åŒæ™‚å®Ÿè¡Œ  
- CPUé›†ç´„çš„å‡¦ç†ã®ä¸¦åˆ—åŒ–ã¨WebSocket I/Oæœ€é©åŒ–
- å®Ÿæ¸¬åŠ¹æœ: 6%ã®é«˜é€ŸåŒ–ç¢ºèªï¼ˆ15ç§’â†’14ç§’/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰

**Phase 2: Battle State Retrieval Parallelization**
- `_retrieve_battles_parallel()`: ãƒãƒˆãƒ«çŠ¶æ…‹å–å¾—ã®å®Œå…¨ä¸¦åˆ—åŒ–
- `_race_async()`: ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å†…ã§ã®ç›´æ¥å®Ÿè¡Œã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›

#### ğŸŒ Multi-Server Infrastructure & Team Caching (37.2x Performance)
**Team Caching System**: ãƒãƒ¼ãƒ èª­ã¿è¾¼ã¿å‡¦ç†ã‚’9.3ms â†’ 0.25msï¼ˆ37.2å€é«˜é€ŸåŒ–ï¼‰
**Multi-Server Distribution**: è¤‡æ•°ã‚µãƒ¼ãƒãƒ¼é–“ã§ã®è‡ªå‹•è² è·åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ï¼ˆ125+ä¸¦åˆ—ç’°å¢ƒå¯¾å¿œï¼‰
**Automated Server Management**: `./scripts/showdown start 5`ã§5ç§’ä»¥å†…ã«5ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆ60å€é«˜é€ŸåŒ–ï¼‰

### State Space & AI Features (2025-07-12 to 2025-07-24)

#### ğŸ§  Complete State Space Normalization (Phases 1-4)
å…¨ç‰¹å¾´é‡ã®æ•°å€¤ã‚¹ã‚±ãƒ¼ãƒ«çµ±ä¸€ã«ã‚ˆã‚‹å­¦ç¿’åŠ¹ç‡å‘ä¸Š:
- **Phase 1**: Pokemonå®Ÿæ•°å€¤æ­£è¦åŒ– [0,337] â†’ [0,1]
- **Phase 2**: Species Embedding 32æ¬¡å…ƒL2æ­£è¦åŒ–
- **Phase 3**: PPå€¤ãƒ»ã‚¿ãƒ¼ãƒ³ãƒ»ã‚«ã‚¦ãƒ³ãƒˆç³»æ­£è¦åŒ–
- **Phase 4**: æŠ€å‘½ä¸­ç‡ãƒ»ãƒ™ãƒ³ãƒãƒã‚±ãƒ¢ãƒ³å®Ÿæ•°å€¤æ­£è¦åŒ–

#### ğŸ¯ Pokemon Species Embedding System
32æ¬¡å…ƒPokemonç¨®æ—embeddingã‚·ã‚¹ãƒ†ãƒ ã§åŠ¹ç‡çš„å­¦ç¿’:
- ç¨®æ—å€¤ã«ã‚ˆã‚‹é‡ã¿åˆæœŸåŒ–ï¼ˆHP/æ”»æ’ƒ/é˜²å¾¡/ç‰¹æ”»/ç‰¹é˜²/ç´ æ—©ã•ï¼‰
- 1025ç¨®æ—å®Œå…¨å¯¾å¿œ + å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿26æ¬¡å…ƒ
- çŠ¶æ…‹ç©ºé–“: 1136 â†’ 1508æ¬¡å…ƒï¼ˆ12Ã—32 embeddingçµ±åˆï¼‰

#### âš”ï¸ Move Embedding & Damage Calculation Integration
**256æ¬¡å…ƒMove Embedding**: æŠ€ã®æˆ¦è¡“çš„ç‰¹æ€§ã‚’åŒ…æ‹¬çš„ã«è¡¨ç¾
- Fixed Parameters: 169æ¬¡å…ƒï¼ˆã‚¿ã‚¤ãƒ—ã€å¨åŠ›ã€å‘½ä¸­ç‡ç­‰ï¼‰
- Learnable Parameters: 87æ¬¡å…ƒï¼ˆæŠ½è±¡çš„æŠ€é–¢ä¿‚å­¦ç¿’ï¼‰
- çŠ¶æ…‹ç©ºé–“æ‹¡å¼µ: 2160æ¬¡å…ƒï¼ˆ1136åŸºæœ¬ + 1024æŠ€embeddingï¼‰

**Complete Damage Calculation**: AIãŒæˆ¦è¡“åˆ¤æ–­ã§æ´»ç”¨å¯èƒ½
- 18Ã—18=324ã‚¨ãƒ³ãƒˆãƒªãƒ¼å®Œå…¨ã‚¿ã‚¤ãƒ—ç›¸æ€§è¡¨
- 288ãƒ€ãƒ¡ãƒ¼ã‚¸ç‰¹å¾´é‡ï¼ˆ4æŠ€Ã—6ç›¸æ‰‹Ã—2ã‚·ãƒŠãƒªã‚ªÃ—6Pokemonï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¨ˆç®—: 2545å›/ç§’ã®é«˜é€Ÿå‡¦ç†

### Exploration & Evaluation Systems (2025-07-20 to 2025-07-21)

#### ğŸ² Îµ-greedy Exploration Strategy
å®Œå…¨ãªÎµ-greedyæ¢ç´¢æˆ¦ç•¥ã§å±€æ‰€è§£å›é¿:
- ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼åˆ†å¸ƒæ··åˆã«ã‚ˆã‚‹ç†è«–çš„æ­£ç¢ºæ€§ç¢ºä¿
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¸›è¡°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆepisode/step modeï¼‰
- CLIè¨­å®šå¯¾å¿œã¨TensorBoardçµ±åˆ

#### ğŸ“Š V1-V3 Evaluation & Logging System
ä½“ç³»çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç®¡ç†ã¨åˆ†æã‚·ã‚¹ãƒ†ãƒ :
- **V1: TensorBoardçµ±ä¸€ãƒ­ã‚°**: 5ã‚«ãƒ†ã‚´ãƒªä½“ç³»çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
- **V2: CSVè‡ªå‹•ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ**: å­¦ç¿’çµ‚äº†æ™‚ã®è‡ªå‹•çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ
- **V3: è¡Œå‹•å¤šæ§˜æ€§åˆ†æ**: Shannon entropyã€KLè·é›¢ã«ã‚ˆã‚‹æ¢ç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ

### Core Architecture Improvements (2025-07-09 to 2025-07-19)

#### ğŸ”„ LSTM & Sequence Learning Optimization
LSTMå­¦ç¿’ã®åŒ…æ‹¬çš„æœ€é©åŒ–:
- Sequence-Based Algorithms: PPO/REINFORCE ã®LSTMç‰¹åŒ–ç‰ˆå®Ÿè£…
- Hidden State Management: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå˜ä½ã§ã®çŠ¶æ…‹ç®¡ç†
- BPTTè¨­å®š: Full Episode/Truncated BPTTé¸æŠå¯èƒ½

#### ğŸ–¥ï¸ GPU Acceleration & Device Support
å¤šãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œGPUåŠ é€Ÿ:
- NVIDIA CUDAã€Apple MPSã€CPUè‡ªå‹•é¸æŠ
- è‡ªå‹•ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡ºã¨é©å¿œçš„ãƒ¡ãƒ¢ãƒªç®¡ç†
- ä¸¦åˆ—å®Ÿè¡Œå®‰å…¨æ€§ã¨ã‚¹ãƒ¬ãƒƒãƒ‰åˆ†é›¢

#### âš™ï¸ Enhanced Configuration & Bug Fixes
**çµ±ä¸€è¨­å®šã‚·ã‚¹ãƒ†ãƒ **: å˜ä¸€`train_config.yml`ã§å…¨è¨“ç·´ã‚·ãƒŠãƒªã‚ªç®¡ç†
**Critical Bug Fixes**: 
- Training Resumeæ©Ÿèƒ½ã®AttributeErrorä¿®æ­£
- FailAndImmuneRewardå®Œå…¨å®Ÿè£…ï¼ˆCustomBattleçµ±åˆï¼‰
- Model Evaluation shape mismatchä¿®æ­£

### Technical Achievements Summary
- **Performance**: 37.2x team caching + 10-15% async processing + 60x server automation
- **State Space**: 2160æ¬¡å…ƒçµ±åˆï¼ˆspecies embedding + move embedding + damage calculationï¼‰
- **Learning**: Îµ-greedy exploration + sequence LSTM + state normalization
- **Infrastructure**: Multi-server distribution + automated management + comprehensive evaluation
- **Reliability**: Complete bug fixes + testing + production-ready implementation

